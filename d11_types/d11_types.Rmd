---
title: "Data types, recoding, and transformations <br> `r emo::ji('minidisc')`"
author: "S. Mason Garrison"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
      slideNumberFormat: ""
---

```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(DT)
library(scales)
```

class: middle

# Why should you care about data types?

---

## Example: Cat lovers

A survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.

```{r message=FALSE}
cat_lovers <- read_csv("data/cat-lovers.csv")
```

```{r echo=FALSE}
cat_lovers
```

---

## Oh why won't you work?!

```{r}
cat_lovers %>%
  summarise(mean_cats = mean(number_of_cats))
```

---

```{r eval=FALSE}
?mean
```

```{r echo=FALSE, caption="Help for mean", out.width="80%", fig.align="center"}
knitr::include_graphics("img/mean-help.png")
```

---

## Oh why won't you still work??!!

```{r}
cat_lovers %>%
  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))
```

---

## Take a breath and look at your data

.question[
What is the type of the `number_of_cats` variable?
]

```{r}
glimpse(cat_lovers)
```

---

## Let's take another look

.small[
```{r echo=FALSE}
cat_lovers %>%
  datatable()
```
]

---

## You might need to babysit your respondents

.midi[
```{r}
cat_lovers %>%
  mutate(number_of_cats = case_when(
    name == "Ginger Clark" ~ 2,
    name == "Doug Bass"    ~ 3,
    TRUE                   ~ as.numeric(number_of_cats)
    )) %>%
  summarise(mean_cats = mean(number_of_cats))
```
]

---

## Always you need to respect data types

```{r}
cat_lovers %>%
  mutate(
    number_of_cats = case_when(
      name == "Ginger Clark" ~ "2",
      name == "Doug Bass"    ~ "3",
      TRUE                   ~ number_of_cats
      ),
    number_of_cats = as.numeric(number_of_cats)
    ) %>%
  summarise(mean_cats = mean(number_of_cats))
```

---

## Now that we know what we're doing...

```{r}
cat_lovers <- cat_lovers %>% #<<
  mutate(
    number_of_cats = case_when(
      name == "Ginger Clark" ~ "2",
      name == "Doug Bass"    ~ "3",
      TRUE                   ~ number_of_cats
      ),
    number_of_cats = as.numeric(number_of_cats)
    )
```

---

## Moral of the story

- If your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.
- Go in and investigate your data, apply the fix, *save your data*, live happily ever after.

---

class: middle

.hand-blue[now that we have a good motivation for]  
.hand-blue[learning about data types in R]

<br>

.hand-blue[let's learn about data types in R!]

---

class: middle

# Data types

---

## Data types in R

- **logical**
- **double**
- **integer**
- **character**
- and some more, but we won't be focusing on those

---

## Logical & character

.pull-left[
**logical** - boolean values `TRUE` and `FALSE`

```{r}
typeof(TRUE)
```
]
.pull-right[
**character** - character strings

```{r}
typeof("hello")
```
]

---

## Double & integer

.pull-left[
**double** - floating point numerical values (default numerical type)

```{r}
typeof(1.335)
typeof(7)
```
]
.pull-right[
**integer** - integer numerical values (indicated with an `L`)

```{r}
typeof(7L)
typeof(1:3)
```
]

---

## Concatenation

Vectors can be constructed using the `c()` function.

```{r}
c(1, 2, 3)
c("Hello", "World!")
c(c("hi", "hello"), c("bye", "jello"))
```

---

## Converting between types

.hand[with intention...]

.pull-left[
```{r}
x <- 1:3
x
typeof(x)
y <- as.character(x)
y
typeof(y)
```
]
--
.pull-right[
```{r}
x <- c(TRUE, FALSE)
x
typeof(x)
y <- as.numeric(x)
y
typeof(y)
```
]

---

## Converting between types

.hand[without intention...]

R will happily convert between various types without complaint when different types of data are concatenated in a vector, and that's not always a great thing!

.pull-left[
```{r}
c(1, "Hello")
c(FALSE, 3L)
```
]
.pull-right[
```{r}
c(1.2, 3L)
c(2L, "two")
```
]

---

## Explicit vs. implicit coercion

Let's give formal names to what we've seen so far:

- **Explicit coercion** is when you call a function like `as.logical()`, `as.numeric()`, `as.integer()`, `as.double()`, or `as.character()`.
- **Implicit coercion** happens when you use a vector in a specific context that expects a certain type of vector. 

---

.midi[
.your-turn[
- [class git repo](https://github.com/DataScience4Psych) > `AE 06 - Hotels + Data types` > open `type-coercion.Rmd` and knit.
- What is the type of the given vectors? First, guess. Then, try it out in R. 
If your guess was correct, great! If not, discuss why they have that type.
]
]

.small[
**Example:** Suppose we want to know the type of `c(1, "a")`. First, I'd look at: 

.pull-left[
```{r}
typeof(1)
```
]
.pull-right[
```{r}
typeof("a")
```
]

and make a guess based on these. Then finally I'd check:
.pull-left[
```{r}
typeof(c(1, "a"))
```
]
]

---

class: middle

# Special values

---

## Special values

- `NA`: Not available
- `NaN`: Not a number
- `Inf`: Positive infinity
- `-Inf`: Negative infinity

--

.pull-left[
```{r}
pi / 0
0 / 0
```
]
.pull-right[
```{r}
1/0 - 1/0
1/0 + 1/0
```
]

---

## `NA`s are special `r emo::ji('snowflake')`s

```{r}
x <- c(1, 2, 3, 4, NA)
```

```{r}
mean(x)
mean(x, na.rm = TRUE)
summary(x)
```

---

## `NA`s are logical

R uses `NA` to represent missing values in its data structures.

```{r}
typeof(NA)
```

---

## Mental model for `NA`s

- Unlike `NaN`, `NA`s are genuinely unknown values
- But that doesn't mean they can't function in a logical way
- Let's think about why `NA`s are logical...

--

.question[
Why do the following give different answers?
]
.pull-left[
```{r}
# TRUE or NA
TRUE | NA
```
]
.pull-right[
```{r}
# FALSE or NA
FALSE | NA
```
]

$\rightarrow$ See next slide for answers...

---

- `NA` is unknown, so it could be `TRUE` or `FALSE`

.pull-left[
.midi[
- `TRUE` or `TRUE` is `TRUE` and `TRUE` or `FALSE` is also `TRUE`, and since both are `TRUE`
```{r}
TRUE | TRUE
FALSE | TRUE
```
]
]

.pull-right[
.midi[
- `FALSE` or `TRUE` is `TRUE` and `FALSE` or `FALSE` is also `FALSE`, so you you can't tell which should be the right answer
```{r}
FALSE | TRUE
FALSE | FALSE
```
]
]

- Doesn't make sense for mathematical operations but make sense in the context of missing data

---

class: middle

# Data classes

---

## Data classes

We talked about *types* so far, next we'll introduce the concept of *classes*

- Vectors are like Lego building blocks
- We stick them together to build more complicated constructs, e.g. *representations of data*
- The **class** attribute relates to the S3 class of an object which determines its behaviour
  - You don't need to worry about what S3 classes really mean, but you can read more about it [here](https://adv-r.hadley.nz/s3.html#s3-classes) if you're curious
- Examples: factors, dates, and data frames
  
---

## Factors

R uses factors to handle categorical variables, variables that have a fixed and known set of possible values

```{r}
x <- factor(c("BS", "MS", "PhD", "MS"))
x
```

.pull-left[
```{r}
typeof(x)
```
]
.pull-right[
```{r}
class(x)
```
]


---

## More on factors

We can think of factors like character (level labels) and an integer (level numbers) glued together

```{r}
glimpse(x)
as.integer(x)
```

---

## Dates

```{r}
y <- as.Date("2020-01-01")
y
typeof(y)
class(y)
```

---

## More on dates

We can think of factors like an integer (the number of days since the origin, 1 Jan 1970) and an integer (the origin) glued together

```{r}
as.integer(y)
as.integer(y) / 365 # roughly 50 yrs
```

---

## Data frames

We can think of data frames like like vectors of equal length glued together

```{r}
df <- data.frame(x = 1:2, y = 3:4)
df
```

.pull-left[
```{r}
typeof(df)
```
]
.pull-right[
```{r}
class(df)
```
]

---

## Lists

Lists are a generic vector container vectors of any type can go in them

```{r}
l <- list(
  x = 1:4,
  y = c("hi", "hello", "jello"),
  z = c(TRUE, FALSE)
)
l
```

---

## Lists and data frames

- A data frame is a special list containing vectors of equal length
- When we use the `pull()` function, we extract a vector from the data frame

```{r}
df

df %>%
  pull(y)
```


---

class: middle

# Working with factors

---

## Read data in as character strings

```{r}
glimpse(cat_lovers)
```

---

## But coerce when plotting

```{r out.width="70%"}
ggplot(cat_lovers, mapping = aes(x = handedness)) +
  geom_bar()
```

---

## Use forcats to manipulate factors

```{r out.width="70%"}
cat_lovers %>%
  mutate(handedness = fct_infreq(handedness)) %>% #<<
  ggplot(mapping = aes(x = handedness)) +
  geom_bar()
```

---

## Come for the functionality

.pull-left[
... stay for the logo
]
.pull-right[
```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/forcats-part-of-tidyverse.png")
```
]
- Factors are useful when you have true categorical data and you want to override the ordering of character vectors to improve display
- They are also useful in modeling scenarios
- The **forcats** package provides a suite of useful tools that solve common problems with factors

---

.small[
.your-turn[
- [class git repo](https://github.com/DataScience4Psych) > `AE 06 - Hotels + Data types` > `hotels-forcats.Rmd` > knit
- Recreate the following. The x-axis first, then, as a stretch goal, the y-axis.
]
]

```{r echo=FALSE, out.width="100%", fig.asp=0.4}
hotels <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")
hotels %>%
  mutate(arrival_date_month = fct_relevel(arrival_date_month, month.name)) %>%
  group_by(hotel, arrival_date_month) %>%   # group by hotel type and arrival month
  summarise(mean_adr = mean(adr)) %>%       # calculate mean_adr for each group
  ggplot(aes(
    x = arrival_date_month,
    y = mean_adr,                           # y-axis is the mean_adr calculated above
    group = hotel,                          # group lines by hotel type
    color = hotel)                          # and color by hotel type
    ) +
  geom_line() +                             # use lines to represent data
  scale_y_continuous(labels = label_dollar()) +
  theme_minimal() +                         # use a minimal theme
  labs(x = "Arrival month",                 # customize labels
       y = "Mean ADR (average daily rate)",
       title = "Comparison of resort and city hotel prices across months",
       subtitle = "Resort hotel prices soar in the summer while city hotel prices remain\nrelatively constant throughout the year",
       color = "Hotel type") +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
```

---

class: middle

# Working with dates

---

## Make a date

.pull-left[
```{r echo=FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics("img/lubridate-not-part-of-tidyverse.png")
```
]
.pull-right[
- **lubridate** is the tidyverse-friendly package that makes dealing with dates a little easier
- It's not one of the *core* tidyverse packages, hence it's installed with `install.packages("tidyverse)` but it's not loaded with it, and needs to be explicitly loaded with `library(lubridate)`
]

---

class: middle

.hand-blue[
we're just going to scratch the surface of working with dates in R here...
]

---

.question[
Calculate and visualize the number of bookings on any given arrival date.
]

```{r}
hotels %>%
  select(starts_with("arrival_"))
```

---

## Step 1. Put together dates.

.midi[
```{r}
library(glue)

hotels %>%
  mutate(
    arrival_date = glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}") #<<
    ) %>% 
  select(starts_with("arrival_"))
```
]

---

## Step 2. Count number of bookings per date.

.midi[
```{r}
hotels %>%
  mutate(arrival_date = glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}")) %>%
  count(arrival_date)
```
]

---

## Step 3. Visualise number of bookings per date.

.midi[
```{r out.width="70%"}
hotels %>%
  mutate(arrival_date = glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}")) %>%
  count(arrival_date) %>%
  ggplot(aes(x = arrival_date, y = n, group = 1)) +
  geom_line()
```
]

---

.hand[zooming in a bit...]

.question[
Why does the plot start with August when we know our data start in July? And why does 10 August come after 1 August?
]

.midi[
```{r out.width="70%", echo=FALSE}
hotels %>%
  mutate(arrival_date = glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}")) %>%
  count(arrival_date) %>%
  slice(1:7) %>%
  ggplot(aes(x = arrival_date, y = n, group = 1)) +
  geom_line()
```
]

---

## Step 1. `REVISED` Put together dates `as dates`.

.midi[
```{r}
library(lubridate)

hotels %>%
  mutate(
    arrival_date = ymd(glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}")) #<<
    ) %>% 
  select(starts_with("arrival_"))
```
]

---

## Step 2. Count number of bookings per date.

.midi[
```{r}
hotels %>%
  mutate(arrival_date = ymd(glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}"))) %>% 
  count(arrival_date)
```
]

---

## Step 3a. Visualise number of bookings per date.

.midi[
```{r out.width="70%"}
hotels %>%
  mutate(arrival_date = ymd(glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}"))) %>% 
  count(arrival_date) %>%
  ggplot(aes(x = arrival_date, y = n, group = 1)) +
  geom_line()
```
]

---

## Step 3b. Visualise using a smooth curve.

.midi[
```{r out.width="70%", message = FALSE}
hotels %>%
  mutate(arrival_date = ymd(glue("{arrival_date_year} {arrival_date_month} {arrival_date_day_of_month}"))) %>% 
  count(arrival_date) %>%
  ggplot(aes(x = arrival_date, y = n, group = 1)) +
  geom_smooth() #<<
```
]


---

middle: Transformations


---

## Log transformation

+ In an intro course, you probably learned to use the most common transformation, the log.

+ The main reason we gave was that it often made positive data more normal.

+ But there's another and perhaps more fundamental reason: 

+ It often leads to differences between samples that we can interpret as a *multiplicative shift*.

+ Some statisticians will go as far as to recommend log transforming positive data by default...

<!--, though by the end of Cleveland's chapter 2, we'll see an example where that backfires.
-->
---

## Power transformations

+ The log transformation doesn't always work -- 
	+ for a start, you can't log zero or a negative number.

+ **Power transformations** allow a wider range of options.

+ Define a power transformation with parameter $\tau$ to be $x^\tau$.

+ Where practical, we'll hugely prefer $\tau = 1$ (no transformation), $\tau = 0$, or possibly $\tau = -1$ (inverse transformation) because they're much easier to interpret.

---

## Box-Cox transformation

+ You might also see Box-Cox transformations: 
	+ these are really just power transformations, 
	+ but re-scaled so that they go continuously to the log transform as $\tau \to 0$ on either side.

+ Let $x$ be the original data value, 
+ and let $f_\tau$ be the Box-Cox transformation with parameter $\tau$. 
+ It is defined as:

$$
f_\tau(x) = \begin{cases}
\frac{x^{\tau} - 1}{\tau} & \tau \ne 0\\
\log x & \tau = 0
\end{cases}
$$

-----

Here's a little picture of the transformations, note how the log transformation floats nicely in between the negative and positive values of $\tau$.

```{r}
x = seq(0, 5, length.out = 100)[-1]
bc_trans = function(x, tau) {
    if(tau == 0){
        return(log(x))
    }
    return((x^tau - 1) / tau)
}
tau_vec = seq(-.4, 1.4, by = .2)
transforms = sapply(tau_vec, function(tau) bc_trans(x, tau))
transforms_melted = melt(transforms)
transforms_for_plotting = transforms_melted %>% mutate(x = x[Var1], tau = tau_vec[Var2])
ggplot(transforms_for_plotting) +
    geom_line(aes(x = x, y = value, color = as.factor(tau))) +
    xlab("Original Data Value") + ylab("Transformed Value")
```


## Let's see how these work on real data

Load `ggplot2`:

```{r}
library(ggplot2)
```

We'll also use an R workspace prepared by Cleveland to use in conjunction with his book. Among other things, this .RData contains the data sets used in the book.

```{r}
load("lattice.RData")
```



We'll use the stereogram fusion time data in `fusion.time`, which contains the group variable `nv.vv` (where "NV" means no visual information and "VV" means visual information) and the quantitative variable `time` (a positive number.)


## Power transformations on fusion time data

Let's try some of the power transformations on the fusion time data --- we would like to see which gives a distribution closest to normal. (Why?)

We'll just look at the VV times for now.

```{r, fig.width = 10, fig.height = 4, dpi = 150}
vv = fusion.time %>% subset(nv.vv == "VV")
tau_vec = seq(-1, 1, by = .25)
transforms = sapply(tau_vec, function(tau) bc_trans(vv$time, tau))
transforms_melted = melt(transforms)
transforms_for_plotting = transforms_melted %>% mutate(tau = tau_vec[Var2])

ggplot(transforms_for_plotting) +
    stat_qq(aes(sample = value)) +
    facet_wrap(~ tau, scales = "free", ncol = 5)
```

Here $\tau$-values of $0$ (the log transformation) and $-0.25$ give the straightest normal QQ plots.

Since it's much, much easier to interpret $\log(x)$ than $x^{-0.25}$, we strongly prefer the log transformation.

## Theoretical reasons for transformations

In your more theoretical statistics courses, you might come across _variance-stabilizing transformations_.

In real data, we often see that the variance depends on the mean.

. . .

For example, if $X$ is distributed $\text{Poisson}(\lambda)$, $E_\lambda(X) = \lambda$, and $\text{Var}_\lambda(X) = \lambda$.

For Poisson random variables, the square root transformation ($\tau =
1/2$) is approximately variance stabilizing, that is, $X^{1/2}$ will
have variance that is about the same no matter what $\lambda$ is.

Conut data often have a Poisson distribution, and so it is often
useful to use a square root transformation for counts.

> The re-expressions most frequently useful for counts are logs and (square) roots. In fact, it is rather hard to find a set of counts that are better analyzed as raw counts than as root counts.

Tukey, EDA, p. 83-84

. . .

If we have random variables $X_i$, with $E(X_i) = \mu$ and $\text{Var}(X_i) = s^2 \mu^2$ (the standard deviation is proportional to the mean), then the log transformation ($\tau = 0$) is approximately variance stabilizing.

. . .



